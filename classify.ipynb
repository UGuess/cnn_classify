{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(input_tensor,train,regularizer):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\"weight\",[5,5,3,32],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable(\"bias\",[32],initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor,conv1_weights,strides=[1,1,1,1],padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1,conv1_biases))\n",
    "        \n",
    "    with tf.name_scope('layer2-pool1'):\n",
    "        pool1 = tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        conv2_weights = tf.get_variable(\"weight\",[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\",[64],initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1,conv2_weights,strides=[1,1,1,1],padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases))\n",
    "        \n",
    "    with tf.name_scope('layer4-pool2'):\n",
    "        pool2 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    with tf.variable_scope('layer5-conv3'):\n",
    "        conv3_weights = tf.get_variable(\"weight\",[3,3,64,128],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv3_biases = tf.get_variable(\"bias\",[128],initializer=tf.constant_initializer(0.0))\n",
    "        conv3 = tf.nn.conv2d(pool2,conv3_weights,strides=[1,1,1,1],padding='SAME')\n",
    "        relu3 = tf.nn.relu(tf.nn.bias_add(conv3,conv3_biases))\n",
    "        \n",
    "    with tf.name_scope('layer6-pool3'):\n",
    "        pool3 = tf.nn.max_pool(relu3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "        \n",
    "    with tf.variable_scope('layer7-conv4'):\n",
    "        conv4_weights = tf.get_variable(\"weight\",[3,3,128,256],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv4_biases = tf.get_variable(\"bias\",[256],initializer=tf.constant_initializer(0.0))\n",
    "        conv4 = tf.nn.conv2d(pool3,conv4_weights,strides=[1,1,1,1],padding='SAME')\n",
    "        relu4 = tf.nn.relu(tf.nn.bias_add(conv4,conv4_biases))\n",
    "        \n",
    "    with tf.name_scope('layer8-pool4'):\n",
    "        pool4 = tf.nn.max_pool(relu4,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "        pool_shape = pool4.get_shape().as_list()\n",
    "        nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "        reshaped = tf.reshape(pool4,[-1,nodes])\n",
    "        \n",
    "    with tf.variable_scope('layer9-fc1'):\n",
    "        fc1_weights = tf.get_variable(\"weight\",[nodes,1024],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None: \n",
    "            tf.add_to_collection('loss',regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable(\"bias\",[1024],initializer=tf.constant_initializer(0.0))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped,fc1_weights)+fc1_biases)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1,0.5)\n",
    "            \n",
    "    with tf.variable_scope('layer10-fc2'):\n",
    "        fc2_weights = tf.get_variable(\"weight\",[1024,512],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None: \n",
    "            tf.add_to_collection('loss',regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable(\"bias\",[512],initializer=tf.constant_initializer(0.0))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1,fc2_weights)+fc2_biases)\n",
    "        if train:\n",
    "            fc2 = tf.nn.dropout(fc2,0.5)\n",
    "            \n",
    "    with tf.variable_scope('layer11-fc3'):\n",
    "        fc3_weights = tf.get_variable(\"weight\",[512,2],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer is not None: \n",
    "            tf.add_to_collection('loss',regularizer(fc3_weights))\n",
    "        fc3_biases = tf.get_variable(\"bias\",[2],initializer=tf.constant_initializer(0.0))\n",
    "        logit = tf.matmul(fc2,fc3_weights)+fc3_biases\n",
    "        \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path,has_glass):\n",
    "    files = os.listdir(path)\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for f in files:\n",
    "        img = cv2.imread(path+f)\n",
    "        img = cv2.resize(img,(100,100))\n",
    "        imgs.append(img)\n",
    "        if has_glass:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return imgs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_path = r\"E:/celeA/has/\"\n",
    "no_path = r\"E:/celeA/no/\"\n",
    "img,label = get_data(has_path,True)\n",
    "no_img,no_label = get_data(no_path,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.extend(no_img)\n",
    "label.extend(no_label)\n",
    "#Y = np.eye(2)[label]\n",
    "X = np.asarray(img,np.float32)\n",
    "Y = np.asarray(label,np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X),len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱顺序 \n",
    "num_example=X.shape[0]\n",
    "arr=np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "X=X[arr]\n",
    "Y=Y[arr]\n",
    "\n",
    "#将所有数据分为训练集和验证集\n",
    "ratio=0.8\n",
    "s=np.int(num_example*ratio)\n",
    "x_train=X[:s]\n",
    "y_train=Y[:s]\n",
    "x_val=X[s:]\n",
    "y_val=Y[s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个函数，按批次取数据\n",
    "def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batch_size)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====epoch 0=====\n",
      "   train loss: 708.027806\n",
      "   train acc: 0.654500\n",
      "   validation loss: 24.983445\n",
      "   validation acc: 0.733871\n",
      "====epoch 1=====\n",
      "   train loss: 17.637438\n",
      "   train acc: 0.742000\n",
      "   validation loss: 14.333073\n",
      "   validation acc: 0.764113\n",
      "====epoch 2=====\n",
      "   train loss: 9.102842\n",
      "   train acc: 0.784625\n",
      "   validation loss: 7.190975\n",
      "   validation acc: 0.792843\n",
      "====epoch 3=====\n",
      "   train loss: 5.600294\n",
      "   train acc: 0.817750\n",
      "   validation loss: 5.324786\n",
      "   validation acc: 0.809476\n",
      "====epoch 4=====\n",
      "   train loss: 3.956611\n",
      "   train acc: 0.830625\n",
      "   validation loss: 3.901650\n",
      "   validation acc: 0.810484\n",
      "====epoch 5=====\n",
      "   train loss: 2.786851\n",
      "   train acc: 0.834625\n",
      "   validation loss: 2.432648\n",
      "   validation acc: 0.849798\n",
      "====epoch 6=====\n",
      "   train loss: 1.912772\n",
      "   train acc: 0.857875\n",
      "   validation loss: 2.039976\n",
      "   validation acc: 0.834173\n",
      "====epoch 7=====\n",
      "   train loss: 1.495205\n",
      "   train acc: 0.861875\n",
      "   validation loss: 1.612010\n",
      "   validation acc: 0.849294\n",
      "====epoch 8=====\n",
      "   train loss: 1.178263\n",
      "   train acc: 0.873000\n",
      "   validation loss: 1.293005\n",
      "   validation acc: 0.873992\n",
      "====epoch 9=====\n",
      "   train loss: 0.938746\n",
      "   train acc: 0.882375\n",
      "   validation loss: 1.017988\n",
      "   validation acc: 0.881048\n",
      "====epoch 10=====\n",
      "   train loss: 0.686786\n",
      "   train acc: 0.895250\n",
      "   validation loss: 0.947005\n",
      "   validation acc: 0.887097\n",
      "====epoch 11=====\n",
      "   train loss: 0.570667\n",
      "   train acc: 0.903125\n",
      "   validation loss: 0.761090\n",
      "   validation acc: 0.883569\n",
      "====epoch 12=====\n",
      "   train loss: 0.526236\n",
      "   train acc: 0.904000\n",
      "   validation loss: 0.631057\n",
      "   validation acc: 0.900202\n",
      "====epoch 13=====\n",
      "   train loss: 0.445763\n",
      "   train acc: 0.904500\n",
      "   validation loss: 0.710272\n",
      "   validation acc: 0.892641\n",
      "====epoch 14=====\n",
      "   train loss: 0.388879\n",
      "   train acc: 0.912125\n",
      "   validation loss: 0.550762\n",
      "   validation acc: 0.897177\n",
      "====epoch 15=====\n",
      "   train loss: 0.336653\n",
      "   train acc: 0.916875\n",
      "   validation loss: 0.539078\n",
      "   validation acc: 0.906250\n",
      "====epoch 16=====\n",
      "   train loss: 0.285435\n",
      "   train acc: 0.922000\n",
      "   validation loss: 0.519806\n",
      "   validation acc: 0.903226\n",
      "====epoch 17=====\n",
      "   train loss: 0.264470\n",
      "   train acc: 0.928375\n",
      "   validation loss: 0.485711\n",
      "   validation acc: 0.898185\n",
      "====epoch 18=====\n",
      "   train loss: 0.224878\n",
      "   train acc: 0.928625\n",
      "   validation loss: 0.398966\n",
      "   validation acc: 0.912298\n",
      "====epoch 19=====\n",
      "   train loss: 0.221985\n",
      "   train acc: 0.935375\n",
      "   validation loss: 0.368688\n",
      "   validation acc: 0.901210\n",
      "====epoch 20=====\n",
      "   train loss: 0.229006\n",
      "   train acc: 0.931375\n",
      "   validation loss: 0.328439\n",
      "   validation acc: 0.917843\n",
      "====epoch 21=====\n",
      "   train loss: 0.177997\n",
      "   train acc: 0.942250\n",
      "   validation loss: 0.364933\n",
      "   validation acc: 0.920867\n",
      "====epoch 22=====\n",
      "   train loss: 0.181524\n",
      "   train acc: 0.942125\n",
      "   validation loss: 0.331867\n",
      "   validation acc: 0.916331\n",
      "====epoch 23=====\n",
      "   train loss: 0.209729\n",
      "   train acc: 0.935875\n",
      "   validation loss: 0.354344\n",
      "   validation acc: 0.914315\n",
      "====epoch 24=====\n",
      "   train loss: 0.174767\n",
      "   train acc: 0.943250\n",
      "   validation loss: 0.342316\n",
      "   validation acc: 0.921371\n",
      "====epoch 25=====\n",
      "   train loss: 0.189281\n",
      "   train acc: 0.945375\n",
      "   validation loss: 0.309794\n",
      "   validation acc: 0.917843\n",
      "====epoch 26=====\n",
      "   train loss: 0.150373\n",
      "   train acc: 0.950250\n",
      "   validation loss: 0.395411\n",
      "   validation acc: 0.924395\n",
      "====epoch 27=====\n",
      "   train loss: 0.141545\n",
      "   train acc: 0.952000\n",
      "   validation loss: 0.316637\n",
      "   validation acc: 0.928931\n",
      "====epoch 28=====\n",
      "   train loss: 0.109361\n",
      "   train acc: 0.960875\n",
      "   validation loss: 0.295568\n",
      "   validation acc: 0.939012\n",
      "====epoch 29=====\n",
      "   train loss: 0.117968\n",
      "   train acc: 0.960875\n",
      "   validation loss: 0.333730\n",
      "   validation acc: 0.933468\n",
      "====epoch 30=====\n",
      "   train loss: 0.133609\n",
      "   train acc: 0.954750\n",
      "   validation loss: 0.329775\n",
      "   validation acc: 0.924899\n",
      "====epoch 31=====\n",
      "   train loss: 0.117960\n",
      "   train acc: 0.962750\n",
      "   validation loss: 0.268775\n",
      "   validation acc: 0.932964\n",
      "====epoch 32=====\n",
      "   train loss: 0.118425\n",
      "   train acc: 0.959250\n",
      "   validation loss: 0.302374\n",
      "   validation acc: 0.923387\n",
      "====epoch 33=====\n",
      "   train loss: 0.115105\n",
      "   train acc: 0.962750\n",
      "   validation loss: 0.319141\n",
      "   validation acc: 0.922883\n",
      "====epoch 34=====\n",
      "   train loss: 0.114118\n",
      "   train acc: 0.961000\n",
      "   validation loss: 0.304060\n",
      "   validation acc: 0.936492\n",
      "====epoch 35=====\n",
      "   train loss: 0.131752\n",
      "   train acc: 0.955375\n",
      "   validation loss: 0.282337\n",
      "   validation acc: 0.925907\n",
      "====epoch 36=====\n",
      "   train loss: 0.143314\n",
      "   train acc: 0.956375\n",
      "   validation loss: 0.314566\n",
      "   validation acc: 0.925403\n",
      "====epoch 37=====\n",
      "   train loss: 0.101576\n",
      "   train acc: 0.967125\n",
      "   validation loss: 0.284395\n",
      "   validation acc: 0.943044\n",
      "====epoch 38=====\n",
      "   train loss: 0.101204\n",
      "   train acc: 0.965875\n",
      "   validation loss: 0.329775\n",
      "   validation acc: 0.936996\n",
      "====epoch 39=====\n",
      "   train loss: 0.136221\n",
      "   train acc: 0.957875\n",
      "   validation loss: 0.316242\n",
      "   validation acc: 0.924899\n",
      "====epoch 40=====\n",
      "   train loss: 0.112704\n",
      "   train acc: 0.966500\n",
      "   validation loss: 0.312066\n",
      "   validation acc: 0.929940\n",
      "====epoch 41=====\n",
      "   train loss: 0.136245\n",
      "   train acc: 0.959750\n",
      "   validation loss: 0.214481\n",
      "   validation acc: 0.922379\n",
      "====epoch 42=====\n",
      "   train loss: 0.086062\n",
      "   train acc: 0.969625\n",
      "   validation loss: 0.196699\n",
      "   validation acc: 0.945060\n",
      "====epoch 43=====\n",
      "   train loss: 0.080634\n",
      "   train acc: 0.974125\n",
      "   validation loss: 0.309749\n",
      "   validation acc: 0.934980\n",
      "====epoch 44=====\n",
      "   train loss: 0.094391\n",
      "   train acc: 0.966250\n",
      "   validation loss: 0.250391\n",
      "   validation acc: 0.941532\n",
      "====epoch 45=====\n",
      "   train loss: 0.083785\n",
      "   train acc: 0.972750\n",
      "   validation loss: 0.246920\n",
      "   validation acc: 0.943548\n",
      "====epoch 46=====\n",
      "   train loss: 0.080894\n",
      "   train acc: 0.972750\n",
      "   validation loss: 0.232953\n",
      "   validation acc: 0.948085\n",
      "====epoch 47=====\n",
      "   train loss: 0.074925\n",
      "   train acc: 0.975125\n",
      "   validation loss: 0.231039\n",
      "   validation acc: 0.948085\n",
      "====epoch 48=====\n",
      "   train loss: 0.074168\n",
      "   train acc: 0.975875\n",
      "   validation loss: 0.240173\n",
      "   validation acc: 0.951613\n",
      "====epoch 49=====\n",
      "   train loss: 0.081533\n",
      "   train acc: 0.971875\n",
      "   validation loss: 0.306369\n",
      "   validation acc: 0.936492\n"
     ]
    }
   ],
   "source": [
    "w = 100\n",
    "h = 100\n",
    "c = 3\n",
    "\n",
    "n_epoch = 50                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "batch_size = 64\n",
    "model_path = r'E:/celeA/model'\n",
    "summary_path = r'E:/celeA/log'\n",
    "\n",
    "with tf.name_scope('input'):\n",
    "    #占位符\n",
    "    x=tf.placeholder(tf.float32,shape=[None,w,h,c],name='x')\n",
    "    y_=tf.placeholder(tf.int32,shape=[None,],name='y_')\n",
    "\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.0001)\n",
    "logits = inference(x,True,regularizer)\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "#(小处理)将logits乘以1赋值给logits_eval，定义name，方便在后续调用模型时通过tensor名字调用输出tensor\n",
    "b = tf.constant(value=1,dtype=tf.float32)\n",
    "logits_eval = tf.multiply(logits,b,name='logits_eval') \n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    with tf.name_scope('corss_entropy'):\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "        tf.summary.scalar('corss_entropy',loss)\n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss,global_step=global_step)\n",
    "    \n",
    "with tf.name_scope('acc'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(logits,1),tf.int32), y_)    \n",
    "    with tf.name_scope('accuary'):\n",
    "        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuary',acc)\n",
    "        \n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(summary_path+'/train',tf.get_default_graph())\n",
    "    val_writer = tf.summary.FileWriter(summary_path+'/val')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epoch):\n",
    "        print(\"====epoch %d=====\"%epoch)\n",
    "        #training\n",
    "        train_loss,train_acc,n_batch = 0,0,0\n",
    "        for x_train_a,y_train_a in minibatches(x_train, y_train, batch_size, shuffle=True):\n",
    "            train_sum,_,err,ac=sess.run([merged,train_op,loss,acc], feed_dict={x: x_train_a, y_: y_train_a})\n",
    "            train_writer.add_summary(train_sum, epoch)\n",
    "            train_loss += err; train_acc += ac; n_batch += 1\n",
    "        print(\"   train loss: %f\" % (np.sum(train_loss)/ n_batch))\n",
    "        print(\"   train acc: %f\" % (np.sum(train_acc)/ n_batch))\n",
    "\n",
    "        #validation\n",
    "        val_loss, val_acc, n_batch = 0, 0, 0\n",
    "        for x_val_a, y_val_a in minibatches(x_val, y_val, batch_size, shuffle=False):\n",
    "            val_sum, err, ac = sess.run([merged,loss,acc], feed_dict={x: x_val_a, y_: y_val_a})\n",
    "            val_writer.add_summary(val_sum, epoch)\n",
    "            val_loss += err; val_acc += ac; n_batch += 1\n",
    "        print(\"   validation loss: %f\" % (np.sum(val_loss)/ n_batch))\n",
    "        print(\"   validation acc: %f\" % (np.sum(val_acc)/ n_batch))\n",
    "        \n",
    "        saver.save(sess,model_path,global_step=global_step)\n",
    "    train_writer.close()\n",
    "    val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
